{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPwuGndIZE4CUAsGx8ymYjA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/crunchdomo/llm_conversation/blob/main/evaluation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rouge_score"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vyZ5DyL25UAV",
        "outputId": "d919e0ac-97b7-4a5d-b787-a9faeb525d28"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting rouge_score\n",
            "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from rouge_score) (1.4.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (from rouge_score) (3.9.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from rouge_score) (2.0.2)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from rouge_score) (1.17.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (1.5.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (4.67.1)\n",
            "Building wheels for collected packages: rouge_score\n",
            "  Building wheel for rouge_score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24934 sha256=f3cf39a94e6a2de6534e1cd33f528f090a49fdaebc98b80fd17aadcf77a24779\n",
            "  Stored in directory: /root/.cache/pip/wheels/1e/19/43/8a442dc83660ca25e163e1bd1f89919284ab0d0c1475475148\n",
            "Successfully built rouge_score\n",
            "Installing collected packages: rouge_score\n",
            "Successfully installed rouge_score-0.1.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "vYXkoA2b3s4Z"
      },
      "outputs": [],
      "source": [
        "from typing import List, Dict\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "import evaluate  # Requires pip install evaluate\n",
        "import json\n",
        "import torch # Import torch for cosine similarity calculation\n",
        "import re\n",
        "\n",
        "class RecipeConversationEvaluator:\n",
        "    def __init__(self, reference_recipe: Dict):\n",
        "        self.reference_steps = reference_recipe['instructions']\n",
        "        self.ingredients = reference_recipe['ingredients']\n",
        "        self.model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "        self.bleu = evaluate.load(\"bleu\")\n",
        "        self.rouge = evaluate.load(\"rouge\")\n",
        "\n",
        "    def evaluate_conversation(self, dialogue: List[Dict]):\n",
        "        chef_messages = [m['content'] for m in dialogue if m['role'] == 'chef']\n",
        "        return {\n",
        "            'ingredient_coverage': self._calculate_ingredient_coverage(chef_messages),\n",
        "            'step_accuracy': self._measure_step_accuracy(chef_messages),\n",
        "            'question_handling': self._assess_question_handling(dialogue),\n",
        "            # Assuming _calculate_semantic_similarity for the overall conversation\n",
        "            # will be implemented or removed if not needed here.\n",
        "            # For now, let's remove it from here if its purpose isn't clear\n",
        "            # or add a basic implementation. Let's add a basic one for messages vs messages.\n",
        "            'semantic_similarity': self._calculate_semantic_similarity(chef_messages, chef_messages) # Example usage, needs refinement based on desired metric\n",
        "        }\n",
        "\n",
        "    # Move the helper methods inside the class\n",
        "    def _calculate_ingredient_coverage(self, messages: List[str]):\n",
        "      # Extract core ingredients (remove quantities/units)\n",
        "      core_ingredients = set()\n",
        "      for ingredient in self.ingredients:\n",
        "          # Remove measurements using regex\n",
        "          base = re.sub(r'^[\\d¼½¾/]+[^a-zA-Z]*', '', ingredient, flags=re.IGNORECASE)\n",
        "          base = re.sub(r'\\(.*?\\)', '', base).strip().lower()  # Remove parentheticals\n",
        "          core_ingredients.add(base)\n",
        "\n",
        "      mentioned = 0\n",
        "      for core in core_ingredients:\n",
        "          if any(core in msg.lower() for msg in messages):\n",
        "              mentioned += 1\n",
        "\n",
        "      return mentioned / len(core_ingredients) if core_ingredients else 0\n",
        "\n",
        "\n",
        "    def _measure_step_accuracy(self, messages: List[str]):\n",
        "        # Ensure the number of chef messages matches the number of reference steps\n",
        "        # or implement a different logic if they can differ.\n",
        "        # For now, assuming a direct 1-to-1 correspondence or partial evaluation.\n",
        "        # Let's handle the case where message count is different from step count.\n",
        "        # We will compare each chef message to the most semantically similar reference step.\n",
        "        if not messages or not self.reference_steps:\n",
        "            return 0.0 # Return 0 if there are no messages or no reference steps\n",
        "\n",
        "        step_similarities = []\n",
        "        ref_embeddings = self.model.encode(self.reference_steps)\n",
        "        msg_embeddings = self.model.encode(messages)\n",
        "\n",
        "        # Calculate similarity between each message and each reference step\n",
        "        cosine_scores = util.pytorch_cos_sim(msg_embeddings, ref_embeddings)\n",
        "\n",
        "        # For each message, find the maximum similarity to any reference step\n",
        "        max_similarities_per_message = torch.max(cosine_scores, dim=1).values.tolist()\n",
        "\n",
        "        # Average the maximum similarities\n",
        "        return sum(max_similarities_per_message) / len(max_similarities_per_message)\n",
        "\n",
        "\n",
        "    def _assess_question_handling(self, dialogue: List[Dict]):\n",
        "        questions = [m['content'] for m in dialogue\n",
        "                    if m['role'] == 'trainee' and '?' in m['content']]\n",
        "        # Ensure responses are paired correctly with questions.\n",
        "        # This logic assumes an immediate response after each question.\n",
        "        # A more robust approach might track conversation flow explicitly.\n",
        "        responses = []\n",
        "        for i, m in enumerate(dialogue[:-1]):\n",
        "            if m['role'] == 'trainee' and '?' in m['content']:\n",
        "                # Check if the next message exists and is a response\n",
        "                if i + 1 < len(dialogue):\n",
        "                    responses.append(dialogue[i+1]['content'])\n",
        "\n",
        "        # Ensure the number of questions and responses are consistent for paired evaluation\n",
        "        # If not assuming 1-to-1, adjust metric calculation\n",
        "        min_len = min(len(questions), len(responses))\n",
        "        questions_paired = questions[:min_len]\n",
        "        responses_paired = responses[:min_len]\n",
        "\n",
        "        return {\n",
        "            'question_response_ratio': len(responses)/len(questions) if questions else 0,\n",
        "            # Calculate semantic similarity only for paired questions and responses\n",
        "            'answer_relevance': self._calculate_semantic_similarity(responses_paired, questions_paired) if questions_paired else 0.0\n",
        "        }\n",
        "\n",
        "    # Define the _calculate_semantic_similarity method\n",
        "    # This method likely calculates the similarity between two lists of strings.\n",
        "    # A simple implementation could be averaging pairwise similarities.\n",
        "    def _calculate_semantic_similarity(self, list1: List[str], list2: List[str]):\n",
        "        if not list1 or not list2:\n",
        "            return 0.0 # Return 0 if either list is empty\n",
        "\n",
        "        # Ensure lists have the same length for pairwise comparison, or adjust logic\n",
        "        min_len = min(len(list1), len(list2))\n",
        "        list1 = list1[:min_len]\n",
        "        list2 = list2[:min_len]\n",
        "\n",
        "        embeddings1 = self.model.encode(list1)\n",
        "        embeddings2 = self.model.encode(list2)\n",
        "\n",
        "        # Calculate pairwise cosine similarities\n",
        "        cosine_scores = util.pytorch_cos_sim(embeddings1, embeddings2)\n",
        "\n",
        "        # For paired lists, we average the diagonal\n",
        "        # If lists can be different lengths or non-paired, the logic needs adjustment\n",
        "        # Assuming paired lists for 'answer_relevance' based on the calling context\n",
        "        # For 'semantic_similarity' in evaluate_conversation, the interpretation is less clear.\n",
        "        # Let's assume pairwise average for this helper method for now.\n",
        "        # If len(list1) == len(list2) and they are meant to be paired:\n",
        "        pairwise_similarities = [cosine_scores[i][i].item() for i in range(min_len)]\n",
        "\n",
        "        # If the lists are not necessarily paired 1-to-1, we might average all scores\n",
        "        # or find the max similarity for each item in list1 against all of list2.\n",
        "        # Based on how it's called in _assess_question_handling, pairwise seems intended.\n",
        "        # For the call in evaluate_conversation with chef_messages vs chef_messages,\n",
        "        # this might not be the desired metric. Let's assume pairwise average is the\n",
        "        # general helper and the caller needs to provide appropriately paired lists.\n",
        "        if not pairwise_similarities:\n",
        "            return 0.0\n",
        "\n",
        "        return sum(pairwise_similarities) / len(pairwise_similarities)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "reference_recipe = {\n",
        "    \"title\": \"Miso-Butter Roast Chicken With Acorn Squash Panzanella\",\n",
        "    \"ingredients\": [\n",
        "        '1 (3½–4-lb.) whole chicken', '2¾ tsp. kosher salt, divided, plus more',\n",
        "        '2 small acorn squash (about 3 lb. total)', '2 Tbsp. finely chopped sage',\n",
        "        '1 Tbsp. finely chopped rosemary', '6 Tbsp. unsalted butter, melted, plus 3 Tbsp. room temperature',\n",
        "        '¼ tsp. ground allspice', 'Pinch of crushed red pepper flakes', 'Freshly ground black pepper',\n",
        "        '⅓ loaf good-quality sturdy white bread, torn into 1\" pieces (about 2½ cups)',\n",
        "        '2 medium apples (such as Gala or Pink Lady; about 14 oz. total), cored, cut into 1\" pieces',\n",
        "        '2 Tbsp. extra-virgin olive oil', '½ small red onion, thinly sliced',\n",
        "        '3 Tbsp. apple cider vinegar', '1 Tbsp. white miso', '¼ cup all-purpose flour',\n",
        "        '2 Tbsp. unsalted butter, room temperature', '¼ cup dry white wine',\n",
        "        '2 cups unsalted chicken broth', '2 tsp. white miso', 'Kosher salt, freshly ground pepper'\n",
        "    ],\n",
        "    \"instructions\": \"\"\"\n",
        "        1. Pat chicken dry with paper towels, season all over with 2 tsp. salt, and tie legs together with kitchen twine. Let sit at room temperature 1 hour.\n",
        "\n",
        "        2. Meanwhile, halve squash and scoop out seeds. Run a vegetable peeler along ridges of squash halves to remove skin. Cut each half into ½\"\"-thick wedges; arrange on a rimmed baking sheet.\n",
        "\n",
        "        3. Combine sage, rosemary, and 6 Tbsp. melted butter in a large bowl; pour half of mixture over squash on baking sheet. Sprinkle squash with allspice, red pepper flakes, and ½ tsp. salt and season with black pepper; toss to coat.\n",
        "\n",
        "        4. Add bread, apples, oil, and ¼ tsp. salt to remaining herb butter in bowl; season with black pepper and toss to combine. Set aside.\n",
        "        Place onion and vinegar in a small bowl; season with salt and toss to coat. Let sit, tossing occasionally, until ready to serve.\n",
        "\n",
        "        5. Place a rack in middle and lower third of oven; preheat to 425°F. Mix miso and 3 Tbsp. room-temperature butter in a small bowl until smooth. Pat chicken dry with paper towels, then rub or brush all over with miso butter. Place chicken in a large cast-iron skillet and roast on middle rack until an instant-read thermometer inserted into the thickest part of breast registers 155°F, 50–60 minutes. (Temperature will climb to 165°F while chicken rests.) Let chicken rest in skillet at least 5 minutes, then transfer to a plate; reserve skillet.\n",
        "\n",
        "        6. Meanwhile, roast squash on lower rack until mostly tender, about 25 minutes. Remove from oven and scatter reserved bread mixture over, spreading into as even a layer as you can manage. Return to oven and roast until bread is golden brown and crisp and apples are tender, about 15 minutes. Remove from oven, drain pickled onions, and toss to combine. Transfer to a serving dish.\n",
        "\n",
        "        7. Using your fingers, mash flour and butter in a small bowl to combine.\n",
        "\n",
        "        8. Set reserved skillet with chicken drippings over medium heat. You should have about ¼ cup, but a little over or under is all good. (If you have significantly more, drain off and set excess aside.) Add wine and cook, stirring often and scraping up any browned bits with a wooden spoon, until bits are loosened and wine is reduced by about half (you should be able to smell the wine), about 2 minutes. Add butter mixture; cook, stirring often, until a smooth paste forms, about 2 minutes. Add broth and any reserved drippings and cook, stirring constantly, until combined and thickened, 6–8 minutes. Remove from heat and stir in miso. Taste and season with salt and black pepper.\n",
        "\n",
        "        9. Serve chicken with gravy and squash panzanella alongside.\n",
        "            \"\"\"\n",
        "}\n",
        "\n",
        "with open(\"/content/llm_conversation.json\", \"r\") as f:\n",
        "    conversation_data = json.load(f)\n",
        "\n",
        "evaluator = RecipeConversationEvaluator(reference_recipe)\n",
        "results = evaluator.evaluate_conversation(conversation_data)\n",
        "print(results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "olN34QMm4er1",
        "outputId": "eb03f23b-9899-4870-b8a2-88a9ce6cfa9c"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'ingredient_coverage': 0.047619047619047616, 'step_accuracy': 0.3325924141332507, 'question_handling': {'question_response_ratio': 1.0, 'answer_relevance': 0.6048608933176313}, 'semantic_similarity': 1.0}\n"
          ]
        }
      ]
    }
  ]
}