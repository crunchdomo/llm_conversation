{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMY6Wy3Nwtu43nNU8ehkE5r",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/crunchdomo/llm_conversation/blob/main/llm_eval.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GTUyinh5gCoz",
        "outputId": "3bc856b2-b63d-42d1-8f0f-a2a7741d9e06"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| model                               |   semantic_similarity |   step_alignment |   missing_steps |\n",
            "|:------------------------------------|----------------------:|-----------------:|----------------:|\n",
            "| meta-llama-Meta-Llama-3-8B-Instruct |                 0.774 |            0.091 |               0 |\n",
            "| gpt-3                               |                 0.771 |            0.091 |               3 |\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "import pandas as pd\n",
        "\n",
        "class RecipeEvaluator:\n",
        "    def __init__(self, reference_path):\n",
        "        self.ref_steps = self._load_steps(reference_path)\n",
        "        self.model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "    def _load_steps(self, path):\n",
        "        with open(path) as f:\n",
        "            data = json.load(f)\n",
        "        return [msg['content'].split('AWAITING')[0].strip()\n",
        "                for msg in data if msg['role'] == 'assistant'\n",
        "                and 'STEP' in msg['content']]\n",
        "\n",
        "    def compare_to_gpt4(self, model_path):\n",
        "        candidate_steps = self._load_steps(model_path)\n",
        "\n",
        "        # Semantic similarity\n",
        "        ref_emb = self.model.encode(self.ref_steps)\n",
        "        can_emb = self.model.encode(candidate_steps)\n",
        "        semantic_sim = util.cos_sim(ref_emb, can_emb).diagonal().mean().item()\n",
        "\n",
        "        # Step alignment score\n",
        "        alignment = sum(1 for ref, can in zip(self.ref_steps, candidate_steps)\n",
        "                       if key_phrases_match(ref, can)) / len(self.ref_steps)\n",
        "\n",
        "        return {\n",
        "            'model': model_path.split('_')[-1].split('.')[0],\n",
        "            'semantic_similarity': round(semantic_sim, 3),\n",
        "            'step_alignment': round(alignment, 3),\n",
        "            'missing_steps': len(self.ref_steps) - len(candidate_steps)\n",
        "        }\n",
        "\n",
        "def key_phrases_match(ref, can):\n",
        "    key_terms = ['salt', 'butter', 'oven', 'roast', 'temperature']\n",
        "    return sum(1 for term in key_terms if term in ref.lower() and term in can.lower()) >= 3\n",
        "\n",
        "# Usage\n",
        "evaluator = RecipeEvaluator('cooking_session_combined_gpt-4-turbo.json')\n",
        "results = []\n",
        "for model in [\n",
        "    'cooking_session_combined_meta-llama-Meta-Llama-3-8B-Instruct.json',\n",
        "    'cooking_session_combined_gpt-3.5-turbo.json'\n",
        "]:\n",
        "    results.append(evaluator.compare_to_gpt4(model))\n",
        "\n",
        "print(pd.DataFrame(results).to_markdown(index=False))\n",
        "\n",
        "\n",
        "from sklearn.metrics import precision_score, recall_score\n",
        "import numpy as np\n",
        "\n",
        "class ExtendedRecipeEvaluator(RecipeEvaluator):\n",
        "    def __init__(self, reference_path):\n",
        "        super().__init__(reference_path)\n",
        "        self.ingredients = {'chicken', 'squash', 'miso', 'butter', 'sage',\n",
        "                           'rosemary', 'allspice', 'pepper flakes', 'apple'}\n",
        "\n",
        "    def compare_to_gpt4(self, model_path):\n",
        "        base_metrics = super().compare_to_gpt4(model_path)\n",
        "        candidate_steps = self._load_steps(model_path)\n",
        "\n",
        "        # Precision/Recall for critical cooking elements\n",
        "        temp_scores = self._temperature_validation(candidate_steps)\n",
        "        tool_scores = self._tool_correctness(candidate_steps)\n",
        "\n",
        "        # Hallucination detection\n",
        "        hallucination = self._detect_ingredient_hallucination(candidate_steps)\n",
        "\n",
        "        return {\n",
        "            **base_metrics,\n",
        "            'temp_accuracy': temp_scores,\n",
        "            'tool_precision': tool_scores,\n",
        "            'ingredient_recall': self._ingredient_recall(candidate_steps),\n",
        "            'hallucination_rate': hallucination,\n",
        "            'temporal_consistency': self._step_order_validation(candidate_steps)\n",
        "        }\n",
        "\n",
        "    def _temperature_validation(self, steps):\n",
        "        ref_temps = [220, 68, 74]  # Celsius values from GPT-4 steps\n",
        "        detected = []\n",
        "        for step in steps:\n",
        "            if '°C' in step:\n",
        "                detected.extend([int(s) for s in step.split() if s.isdigit()])\n",
        "        return len(set(detected) & set(ref_temps)) / len(ref_temps)\n",
        "\n",
        "    def _tool_correctness(self, steps):\n",
        "        tools = {'oven', 'skillet', 'peeler', 'thermometer', 'whisk'}\n",
        "        present = [tool for step in steps\n",
        "                  for tool in tools if tool in step.lower()]\n",
        "        return len(set(present)) / len(tools)\n",
        "\n",
        "    def _ingredient_recall(self, steps):\n",
        "        present = [ing for step in steps\n",
        "                  for ing in self.ingredients if ing in step.lower()]\n",
        "        return len(set(present)) / len(self.ingredients)\n",
        "\n",
        "    def _detect_ingredient_hallucination(self, steps):\n",
        "        foreign_ings = set()\n",
        "        for step in steps:\n",
        "            words = set(step.lower().split())\n",
        "            foreign_ings.update(words - self.ingredients)\n",
        "        return len(foreign_ings) / len(steps)\n",
        "\n",
        "    def _step_order_validation(self, steps):\n",
        "        key_verbs = ['pat', 'preheat', 'roast', 'toss', 'serve']\n",
        "        order_score = sum(1 for gen, ref in zip(steps, self.ref_steps)\n",
        "                         if any(v in gen.lower() for v in key_verbs))\n",
        "        return order_score / len(key_verbs)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install levenshtein\n",
        "import json\n",
        "import re\n",
        "import pandas as pd\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "from Levenshtein import ratio as levenshtein_ratio\n",
        "from sklearn.metrics import precision_score\n",
        "\n",
        "class RecipeComparator:\n",
        "    def __init__(self, gpt4_path):\n",
        "        self.ref_steps = self._load_steps(gpt4_path)\n",
        "        self.model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "        self.ingredients = {\n",
        "            'chicken', 'squash', 'miso', 'butter', 'sage',\n",
        "            'rosemary', 'allspice', 'pepper flakes', 'apple'\n",
        "        }\n",
        "        self.tools = {'oven', 'skillet', 'peeler', 'thermometer', 'whisk'}\n",
        "        self.temps = {220, 68, 74}  # Celsius temps from GPT-4 steps\n",
        "\n",
        "    def _load_steps(self, path):\n",
        "        with open(path) as f:\n",
        "            data = json.load(f)\n",
        "        return [msg['content'].split('AWAITING')[0].strip()\n",
        "               for msg in data if msg['role'] == 'assistant'\n",
        "               and 'STEP' in msg['content']]\n",
        "\n",
        "    def compare_to_gpt4(self, model_path):\n",
        "        candidate_steps = self._load_steps(model_path)\n",
        "\n",
        "        return {\n",
        "            'model': model_path.split('_')[-1].split('.')[0],\n",
        "            'semantic_similarity': self._semantic_sim(candidate_steps),\n",
        "            'step_completeness': self._step_completeness(candidate_steps),\n",
        "            'temp_accuracy': self._temp_validation(candidate_steps),\n",
        "            'ingredient_recall': self._ingredient_recall(candidate_steps),\n",
        "            'tool_precision': self._tool_check(candidate_steps),\n",
        "            'measurement_consistency': self._unit_check(candidate_steps),\n",
        "            'hallucination_score': self._hallucination(candidate_steps)\n",
        "        }\n",
        "\n",
        "    def _semantic_sim(self, steps):\n",
        "        ref_emb = self.model.encode(self.ref_steps)\n",
        "        can_emb = self.model.encode(steps)\n",
        "        return util.cos_sim(ref_emb, can_emb).diagonal().mean().item()\n",
        "\n",
        "    def _step_completeness(self, steps):\n",
        "        return len(steps)/len(self.ref_steps) if len(steps) <= len(self.ref_steps) else 1.0\n",
        "\n",
        "    def _temp_validation(self, steps):\n",
        "        detected = []\n",
        "        for step in steps:\n",
        "            if '°C' in step:\n",
        "                temps = [int(s.replace('°C','').strip())\n",
        "                        for s in step.split()\n",
        "                        if s.startswith('°C') and s[1:].isdigit()]\n",
        "                detected.extend(temps)\n",
        "        return len(set(detected) & self.temps)/len(self.temps)\n",
        "\n",
        "    def _ingredient_recall(self, steps):\n",
        "        present = {ing for step in steps\n",
        "                  for ing in self.ingredients if ing in step.lower()}\n",
        "        return len(present)/len(self.ingredients)\n",
        "\n",
        "    def _tool_check(self, steps):\n",
        "        present = {tool for step in steps\n",
        "                  for tool in self.tools if tool in step.lower()}\n",
        "        return len(present)/len(self.tools)\n",
        "\n",
        "    def _unit_check(self, steps):\n",
        "        metric_units = {'g', 'kg', 'ml', 'l', 'tsp', 'tbsp'}\n",
        "        return sum(1 for step in steps\n",
        "                  if any(u in step for u in metric_units))/len(steps)\n",
        "\n",
        "    def _hallucination(self, steps):\n",
        "        foreign = set()\n",
        "        for step in steps:\n",
        "            words = set(re.findall(r'\\b\\w+\\b', step.lower()))\n",
        "            foreign.update(words - self.ingredients - self.tools)\n",
        "        return len(foreign)/len(steps)\n",
        "\n",
        "# Usage\n",
        "comparator = RecipeComparator('cooking_session_combined_gpt-4-turbo.json')\n",
        "\n",
        "results = []\n",
        "for model in [\n",
        "    'cooking_session_combined_meta-llama-Meta-Llama-3-8B-Instruct.json',\n",
        "    'cooking_session_combined_gpt-3.5-turbo.json'\n",
        "]:\n",
        "    results.append(comparator.compare_to_gpt4(model))\n",
        "\n",
        "df = pd.DataFrame(results)\n",
        "print(df.round(2).to_markdown(index=False))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WTPhxKENghnt",
        "outputId": "cafed34a-3cec-4ffe-b803-7de13968e464"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting levenshtein\n",
            "  Using cached levenshtein-0.27.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
            "Collecting rapidfuzz<4.0.0,>=3.9.0 (from levenshtein)\n",
            "  Downloading rapidfuzz-3.13.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Downloading levenshtein-0.27.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (161 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m161.7/161.7 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading rapidfuzz-3.13.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m58.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: rapidfuzz, levenshtein\n",
            "Successfully installed levenshtein-0.27.1 rapidfuzz-3.13.0\n",
            "| model                               |   semantic_similarity |   step_completeness |   temp_accuracy |   ingredient_recall |   tool_precision |   measurement_consistency |   hallucination_score |\n",
            "|:------------------------------------|----------------------:|--------------------:|----------------:|--------------------:|-----------------:|--------------------------:|----------------------:|\n",
            "| meta-llama-Meta-Llama-3-8B-Instruct |                  0.77 |                1    |               0 |                   1 |              0.8 |                         1 |                 23.55 |\n",
            "| gpt-3                               |                  0.77 |                0.73 |               0 |                   1 |              0.8 |                         1 |                 22.38 |\n"
          ]
        }
      ]
    }
  ]
}